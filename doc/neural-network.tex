\section{Neural Network}

In this project, a fully connected neural network (FCN) is used for the binary classification task of distinguishing between birds and cats from the CIFAR-10 dataset.

While convolutional neural networks (CNNs) are typically more effective for image classification, the choice of an FCN is intentional and aligned with the experimental goals. The primary objective of this work is to study the effects of data poisoning attacks on neural networks, rather than to achieve state-of-the-art classification performance.

\subsection{Fully Connected Neural Network for Binary Classification}

The \texttt{Net} class defines a fully connected neural network using PyTorch for binary classification of CIFAR-10 images (bird vs. cat). The architecture and functionality are described as follows:

\begin{itemize}
    \item \textbf{Architecture:}
    \begin{itemize}
        \item Input images of size $3 \times 32 \times 32$ are flattened into vectors of size 3072.
        \item The model consists of three fully connected (\texttt{Linear}) layers:
        \begin{itemize}
            \item FC1: 3072 $\rightarrow$ 512 with ReLU activation.
            \item FC2: 512 $\rightarrow$ 256 with ReLU activation.
            \item FC3: 256 $\rightarrow$ 1 output (a single logit).
        \end{itemize}
    \end{itemize}

    \item \textbf{Forward Pass:}
    The \texttt{forward} method defines how input data flows through the network layers. After flattening, it is passed through the dense layers with ReLU activations, producing a single output used for binary classification.

    \item \textbf{Training (\texttt{train\_network} method):}
    \begin{itemize}
        \item The model is trained using a specified loss function and optimizer.
        \item For each epoch, the model performs a forward pass, computes the loss, backpropagates the gradients, and updates weights.
        \item After each epoch, the average loss is printed and recorded.
        \item The trained model is saved to the path defined by \texttt{TRAINED\_MODEL\_PATH}.
    \end{itemize}

    \item \textbf{Evaluation (\texttt{test} method):}
    \begin{itemize}
        \item The trained model is loaded and evaluated on the test dataset.
        \item Predictions are passed through a sigmoid function to obtain probabilities.
        \item Binary predictions are computed by thresholding at 0.5.
        \item The method calculates and prints the overall test accuracy.
    \end{itemize}
\end{itemize}

This architecture is suitable for simple binary classification tasks using flattened image vectors. While not as powerful as convolutional networks for image data, it provides a straightforward and interpretable baseline.
\\We chose a neural network with three fully connected layers. This simple design is strong enough for our task and helps us focus on the main goal: studying how poisoning affects the learning process.




\begin{itemize}
    \item \textbf{More than one layer helps the model learn better:} One layer can only learn basic patterns. Using more layers helps the model understand more difficult patterns in the images.
    
    \item \textbf{Each layer has a different job:}
    \begin{itemize}
        \item The first layer finds simple things like color and edges.
        \item The second layer builds more useful shapes from these.
        \item The third layer makes the final decision: bird or cat.
    \end{itemize}
    
    \item \textbf{Simple is enough for our goal:} Because we want to study how poisoning changes training, we do not need a very advanced model.
    
    \item \textbf{Bigger networks are not needed here:} CIFAR-10 images are small, and we only have two classes. A bigger network would be harder to train and would not improve the results much.
\end{itemize}

In the listing \ref{lst:NeuralNetwork} is shown the class that implements the discussed functionalities:

\begin{minipage}{\linewidth}
    \begin{lstlisting}[caption=NeuralNetwork, label={lst:NeuralNetwork}]
        class Net(nn.Module):
        
            def __init__(self):
                
                super().__init__()
                self.flatten = nn.Flatten()
                self.fc1 = nn.Linear(3 * 32 * 32, 512)
                self.fc2 = nn.Linear(512, 256)
                self.fc3 = nn.Linear(256, 1)
                self.relu = nn.ReLU()
                
    \end{lstlisting}
\end{minipage}

\subsection{Understanding the Forward Pass of the Neural Network}

When an image is passed through the fully connected neural network (FCN), several operations are applied step by step. Each operation transforms the data until a final decision (output) is made. The following explains each part of the process in detail.

\begin{enumerate}
    \item \textbf{Flatten Layer:} 
    Images from CIFAR-10 are 3-dimensional (3 channels for color, 32Ã—32 pixels). Before using them in a fully connected network, the image is converted to a 1D vector of length $3 \times 32 \times 32 = 3072$. This process is called \textit{flattening}.

    \item \textbf{First Linear Layer (FC1):}
    This layer takes the 3072 input values and maps them to 512 output values. This is done using a \textit{linear transformation}, which means each output is a weighted sum of the inputs, plus a bias term.

    \item \textbf{ReLU Activation:} 
    After the linear transformation, we apply a function called \textit{ReLU} (Rectified Linear Unit). This function replaces all negative values with zero. It helps the network learn non-linear patterns.

    \item \textbf{Second Linear Layer (FC2) + ReLU:} 
    The same process is repeated: the 512 values are transformed into 256, and ReLU is applied again.

    \item \textbf{Third Linear Layer (FC3):} 
    Finally, the 256 values are transformed into a single number. This number is called a \textit{logit}, which represents how strongly the model thinks the image belongs to the positive class (e.g., a bird).

    \item \textbf{Sigmoid Activation (at test time):} 
    In the testing phase, we apply the \textit{sigmoid} function to the output. This converts the logit into a probability between 0 and 1. If the probability is greater than 0.5, the image is classified as a bird; otherwise, it is classified as a cat.
\end{enumerate}

\begin{minipage}{\linewidth}
    \begin{lstlisting}[caption=Forward and Train, label={lst:ForwardTrain}]
        def forward(self, x):
        
                x = self.flatten(x)
                x = self.relu(self.fc1(x))
                x = self.relu(self.fc2(x))
                x = self.fc3(x)
                return x
        
        def train_network(self, trainloader, optimizer, criterion, num_epochs=100):
                
            self.train()
            train_losses = []
            for epoch in range(num_epochs):
                running_loss = 0.0
                self.train()
                for i, data in enumerate(trainloader, 0):
                    inputs, labels = data
                    labels = labels.float().unsqueeze(1)
                    optimizer.zero_grad()
                    outputs = self(inputs)
                    loss = criterion(outputs, labels)
                    loss.backward()
                    optimizer.step()
                    running_loss += loss.item()
                # Record average training loss
                avg_train_loss = running_loss / len(trainloader)
                train_losses.append(avg_train_loss)
                print(f"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}")
            torch.save(self.state_dict(), TRAINED_MODEL_PATH)
            return train_losses
    \end{lstlisting}
\end{minipage}
                
\subsection{The \texttt{test} Method}

After training the neural network, we need to evaluate how well it performs on new data. The code in \ref{lst:test} shows the method responsible for this. It follows these steps:

\begin{itemize}
    \item \textbf{Loads the trained model:} It uses the weights saved during training to ensure the model is in its best state.
    
    \item \textbf{Switches to evaluation mode:} This disables certain training behaviors to make the results more stable.

    \item \textbf{Makes predictions on the test set:} The model runs over the test data and outputs values (logits).

    \item \textbf{Applies the sigmoid function:} This converts the logits to probabilities between 0 and 1.

    \item \textbf{Decides the predicted class:} If the probability is above 0.5, the model predicts class 1 (bird); otherwise, class 0 (cat).

    \item \textbf{Calculates accuracy:} The method compares predictions with the true labels and prints the percentage of correct predictions.
\end{itemize}

\begin{minipage}{\linewidth}
    \begin{lstlisting}[caption=Test, label={lst:test}]
            def test(self, testloader):
                self.eval()
                self.load_state_dict(torch.load(PATH))
                correct = 0
                total = 0
                with torch.no_grad():
                    for data in testloader:
                        images, labels = data
                        labels = labels.float().unsqueeze(1)  
        
                        outputs = self(images)
                        probs = torch.sigmoid(outputs)
                        predicted = (probs > 0.5).float()
        
                        total += labels.size(0)
                        correct += (predicted == labels).sum().item()
        
                accuracy = 100 * correct / total
                print(f'Accuracy of the network on the test images: {accuracy:.4f}%')
    \end{lstlisting}
\end{minipage}