\begin{abstract}
    This work presents an experiment where a poisoning attack is applied to a binary classifier trained on the CIFAR-10 dataset. 
    The model classifies images as either "Bird" or "Cat". Inspired by the Poison Frogs attack (Algorithm 1), the goal is to change a "Cat" image so that the model misclassifies it as "Bird".
\end{abstract}

\section{Introduction}
Machine learning models are vulnerable to different types of attacks. This work shows the \textsf{Targeted Clean-Label Poisoning Attacks}, which adds examples to the training set to manipulate the behavior of the model at test time.
This attack does not require control over the labeling of training data (\textbf{clean-label}). It is \textbf{targeted}, which means it can control the behavior of the classifier for a specific test instance without reducing the overall performance of the classifier.
This type of attack can be dangerous. For example, an attacker could add a poisoned image (properly labeled) to a training set for a face recognition engine, and control the identity of a chosen person at test time. 
Because the attacker does not need to control the labeling process, poisoned images can be added to the training set just by uploading them to the internet and waiting for them to be collected by a data scraping tool. 
The core goal of this work is to show the process \textsf{to poison} a "Cat" image in such a way that the model classifies it as a "Bird" while the poisoned image remains indistinguishable to the human eye.
