\section{The attack}
We apply a poisoning attack based on the Poison Frogs method, introduced in the paper \textit{Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks}.\\
This is a \textbf{clean-label} attack, meaning that the poison data appears normal and has the correct class label. This makes the attack difficult to detect.\\
The attack uses a strategy called \textbf{feature collision}. The idea is to take a base image (a cat) and carefully modify it so that, inside the neural network's hidden layers, it becomes close to a different image (the \textbf{target}, a bird). The poison image still looks like a cat, but it behaves like the target in feature space. This confuses the model during training.
\\
To generate the poison, we follow \textbf{Algorithm \ref{alg:poison_target}} from the original paper. This algorithm uses a pre-trained network to guide the poison image toward the target in the feature space.\\
After generating the poison image, we insert it into the training set. If the attack is successful, the model will misclassify the target image after training, even though the training data appears clean.\\

\begin{algorithm}[H]
    \caption{Poisoning Example Generation}
    \label{alg:poisoning_example}
    \KwIn{target instance $t$, base instance $b$, learning rate $\lambda$}
    \BlankLine
    Initialize: $x_0 \leftarrow b$\;
    Define: $L_p(x) = ||f(x) - f(t)||^2$\;
    \For{$i = 1$ \KwTo $maxIters$}{
        Forward step: $\hat{x}_i = x_{i-1} - \lambda \nabla_x L_p(x_{i-1})$\;
        Backward step: $x_i = (\hat{x}_i + \lambda \beta b) / (1 + \beta \lambda)$\;
    }
    \label{alg:poison_target}
\end{algorithm}
\subsection{Attack overview}

The idea is to generate a \textbf{poison image} that is labeled correctly and looks like a normal image, but during training, it causes the model to misclassify a \textbf{target image}.

The method starts from a \textit{base instance} and gradually moves it closer to a \textit{target instance} in the model's feature space, while keeping it visually similar to the base.

\subsubsection*{1. The Role and Computation of $\beta$}

The parameter $\beta$ is used to control how close the poison image stays to the base image during optimization. If $\beta$ is too low, the poison moves too far from the base and may no longer look natural. If it is too high, the poison may not get close enough to the target in feature space.
It is calculated using the following formula:

\[
\beta = \beta_0 \cdot \left( \frac{d_{\text{feat}}^2}{d_{\text{input}}^2} \right)
\]

where:
\begin{itemize}
    \item $\beta_0$ is a constant (typically set to 0.25),
    \item $d_{\text{feat}}$ is the dimensionality of the feature representation in the model for the Neural Network used is the output of FC2 that is equals to 256.
    \item $d_{\text{input}}$ is the number of input pixels of the image ($3 \times 32 \times 32 = 3072$).
\end{itemize}

\subsubsection*{2. Poison Generation Algorithm}

We generate a poisoned example by performing gradient-based optimization. The goal is to make the output of the model for the poison image close to the output for the target image in feature space. At the same time, we add a correction step that pulls the image back toward the base.

\begin{itemize}
    \item $f(\cdot)$ be the modelâ€™s feature extractor (or full model),
    \item $t$ be the target instance,
    \item $b$ be the base instance,
    \item $x$ be the poison image (starting from $x = b$),
    \item $\lambda$ be the learning rate,
    \item $\beta$ the parameter described above.
\end{itemize}

We minimize the loss:

\[
L_p(x) = \| f(x) - f(t) \|^2
\]

Each iteration of the algorithm consists of:
\begin{enumerate}
    \item Compute the gradient of $L_p(x)$ with respect to $x$.
    \item Perform a forward step:
    \[
    x_{\text{forward}} = x - \lambda \cdot \nabla L_p(x)
    \]
    \item Perform a backward (correction) step:
    \[
    x \leftarrow \frac{x_{\text{forward}} + \lambda \cdot \beta \cdot b}{1 + \lambda \cdot \beta}
    \]
    \item Clamp the values of $x$ so that the pixels stay in the valid range $[0, 1]$.
\end{enumerate}

This process is repeated for a fixed number of iterations (e.g., 100). At the end, we obtain a poison image $x$ that visually resembles the base $b$ but behaves like the target $t$ inside the model.
Listing \ref{lst:generate_poison} shows the python implementation
\begin{minipage}{\linewidth}
    \begin{lstlisting}[caption=Generate Poison algorithm, label={lst:generate_poison}]
        def generate_poison(model, target_instance, base_instance, learning_rate=0.1, max_iters=100, beta=0.1, device="cuda"):
    
            model = model.to(devitarget_instance = target_instance.to(device)
            base_instance = base_instance.to(device)

            # Initialize x with the base instance and enable gradient tracking
            x = base_instance.clone().detach().
            requires_grad_(True).to(device)

            # Loss function (Lp(x) = || f(x) - f(t) ||^2)
            def Lp(x):
                output_x = model(x)
                output_target = model(target_instance)
                loss = torch.norm(output_x - output_target, p=2) ** 2
                return loss

            for i in range(max_iters):
                # x requires gradients every iteration
                x.requires_grad_()  
                loss = Lp(x)
                # gradients computation
                gradients = torch.autograd.grad(loss, x)[0]
                # Forward step: update x using gradient descent
                with torch.no_grad():
                    xbi = x - learning_rate * gradients  # Perform update on x
                    # Backward step: update x with the base instance influence
                    x = (xbi + learning_rate * beta * base_instance) / (1 + beta * learning_rate)
                    # Ensure x stays within the valid image range ([0, 1])
                    x = torch.clamp(x, 0, 1)
            # Return the final poisoned image, detached from the computation graph
            return x.detach()
    \end{lstlisting}
\end{minipage}
