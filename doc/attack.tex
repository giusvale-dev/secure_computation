\section{The attack}
We apply a poisoning attack based on the Poison Frogs method, introduced in the paper \textit{Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks}.\\
This is a \textbf{clean-label} attack, meaning that the poison data appears normal and has the correct class label. This makes the attack difficult to detect.\\
To generate the poison, we follow \textbf{Algorithm \ref{alg:poison_target}} from the original paper. This algorithm uses a pre-trained network to guide the poison image toward the target in the feature space.\\

\begin{algorithm}[H]
    \caption{Poisoning Example Generation}
    \label{alg:poisoning_example}
    \KwIn{target instance $t$, base instance $b$, learning rate $\lambda$}
    \BlankLine
    Initialize: $x_0 \leftarrow b$\;
    Define: $L_p(x) = ||f(x) - f(t)||^2$\;
    \For{$i = 1$ \KwTo $maxIters$}{
        Forward step: $\hat{x}_i = x_{i-1} - \lambda \nabla_x L_p(x_{i-1})$\;
        Backward step: $x_i = (\hat{x}_i + \lambda \beta b) / (1 + \beta \lambda)$\;
    }
    \label{alg:poison_target}
\end{algorithm}
\subsection{Algorithm overview}
\subsubsection*{1. The Role and Computation of $\beta$}

The parameter $\beta$ is used to control how close the poison image stays to the base image during optimization. If $\beta$ is too low, the poison moves too far from the base and may no longer look natural. If it is too high, the poison may not get close enough to the target in feature space.
It is calculated using the following formula:

\[
\beta = \beta_0 \cdot \left( \frac{d_{\text{feat}}^2}{d_{\text{input}}^2} \right)
\]

where:
\begin{itemize}
    \item $\beta_0$ is a constant (typically set to 0.25),
    \item $d_{\text{feat}}$ is the dimensionality of the feature representation in the model.
    \item $d_{\text{input}}$ is the number of input pixels of the image ($3 \times 32 \times 32 = 3072$).
\end{itemize}

\subsubsection*{2. Poison Generation Algorithm}

We generate a poisoned example by performing gradient-based optimization. The goal is to make the output of the model for the poison image close to the output for the target image in feature space. At the same time, we add a correction step that pulls the image back toward the base.

\begin{itemize}
    \item $f(\cdot)$ the model,
    \item $t$ be the target instance,
    \item $b$ be the base instance,
    \item $x$ be the poison image (starting from $x = b$),
    \item $\lambda$ be the learning rate,
    \item $\beta$ the parameter described above.
\end{itemize}

We minimize the loss:

\[
L_p(x) = \| f(x) - f(t) \|^2
\]

Each iteration of the algorithm consists of:
\begin{enumerate}
    \item Compute the gradient of $L_p(x)$ with respect to $x$.
    \item Perform a forward step:
    \[
    x_{\text{forward}} = x - \lambda \cdot \nabla L_p(x)
    \]
    \item Perform a backward (correction) step:
    \[
    x \leftarrow \frac{x_{\text{forward}} + \lambda \cdot \beta \cdot b}{1 + \lambda \cdot \beta}
    \]
    \item Clamp the values of $x$ so that the pixels stay in the valid range $[0, 1]$.
\end{enumerate}

This process is repeated for a fixed number of iterations (e.g., 100). At the end, we obtain a poison image $x$ that visually resembles the base $b$ but behaves like the target $t$ inside the model.

\subsection{Attack steps}

\subsubsection{Dataset Preparation}
The code uses a filtered version of the CIFAR-10 dataset, where only two classes are kept: "Cat" (label 0) and "Bird" (label 1). The data is loaded and preprocessed using the \texttt{CIFAR10CatBird} class, which includes normalization and resizing.

\subsubsection{Model Initialization}
A MobileNetV2 model is used as a binary classifier. It starts with pretrained weights from ImageNet and replaces the final classification layer to output two classes. The feature extractor can be frozen to keep the learned ImageNet features.

\subsubsection{Training and Evaluation}
The model is trained using standard cross-entropy loss and Adam optimizer. Before and after training, it is evaluated to measure accuracy on the test set. This gives a baseline before applying any poisoning attack.

\subsubsection{Poisoning Algorithm}
The poisoning step follows the Poison Frogs method:

\begin{itemize}
    \item A "base" image of a Cat and a "target" image of a Bird are selected from the test set.
    \item Both images are denormalized for visualization and then normalized again.
    \item The algorithm modifies the Cat image so that its internal model representation becomes similar to the Bird image. This is done using gradient descent over multiple iterations.
    \item The final poisoned image still looks like a Cat but is predicted by the model as a Bird.
\end{itemize}

\subsubsection{Adding the Poisoned Image}
After poisoning, the image is added to the training dataset with its correct label ("Cat"). This keeps the dataset clean, since labels are not changed.

\subsubsection{Retraining with Poisoned Data}
The model is retrained for one epoch on the new training set, which includes the poisoned image. After training, the model is tested again to evaluate the effect of the poisoned image on its predictions.