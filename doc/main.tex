\documentclass{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}

\lstset{
  language=Python,
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{green!60!black},
  numbers=left,
  numberstyle=\tiny\color{gray},
  stepnumber=1,
  numbersep=5pt,
  showstringspaces=false,
  tabsize=4,
  breaklines=true,
  breakatwhitespace=true,
  frame=single
}

\author{Giuseppe Valente \\ Sapienza University of Rome \\ Secure Computation\\}
\date{2024/2025}
\title{Attack binary classifier using clean label poisoning feature collision on CIFAR-10 Dataset\\}
\begin{document}

\maketitle

\newpage

\tableofcontents

\newpage

\include{abstract}
\include{cifar-10}
\include{neural-network}


\section{Methodology}
The poisoning process follows the strategy described in the Poison Frogs paper. First, we train a binary classifier on clean CIFAR-10 data. Then, we choose one "Bird" image as the \textit{target}. We also select five "Cat" images to be poisoned.

The poisoning algorithm works by modifying each base image (Cat) so that the modelâ€™s internal output for the poisoned image becomes close to the output for the target image (Bird). This is done by minimizing the squared distance between their logits (model outputs before activation).

After poisoning, we retrain the model with the modified dataset and check if the poisoned images are now misclassified as "Bird".

\section{Experiment}
We use a neural network with three fully connected layers:
\begin{itemize}
    \item Input: 3072 features (flattened 3x32x32 image)
    \item FC1: 512 neurons with ReLU
    \item FC2: 256 neurons with ReLU
    \item Output: 1 neuron (logit for binary classification)
\end{itemize}

The CIFAR-10 dataset is split into 80\% training and 20\% testing. We apply a binary label where classes 2--7 are "Bird" (label 1), and the others are "Cat" (label 0). The model is trained using SGD with momentum, and binary cross-entropy loss.

After the clean training, we apply the Poison Frogs method with the following settings:
\begin{itemize}
    \item Learning rate: 0.01 for poison generation
    \item Number of poisoned samples: 5
    \item Iterations: 1000 per poisoned sample
\end{itemize}

\section{Results}
After clean training, the model achieved an accuracy of \textbf{86.37\%} on the test set. Five poisoned images were generated, and all five were misclassified as "Bird", even though their true label was "Cat". This shows the success of the poisoning attack.

Then we retrained the model with the poisoned dataset. After 10 more epochs, the model reached a slightly higher test accuracy of \textbf{86.59\%}, even though it now wrongly classified the poisoned images.

Figure~\ref{fig:poisoned} shows a comparison between the base and poisoned images. Figure~\ref{fig:misclassified} displays the poisoned images that were misclassified.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{data/base_vs_poisoned.png}
    \caption{Base vs. Poisoned images. Left: original Cat. Right: modified (poisoned) version.}
    \label{fig:poisoned}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{data/missclassified.png}
    \caption{Poisoned images misclassified as "Bird".}
    \label{fig:misclassified}
\end{figure}

\section{Conclusion}
This experiment demonstrates that it is possible to manipulate a binary classifier by poisoning only a small number of training samples. The Poison Frogs method is effective at making the model learn wrong patterns that match the target image.

Although the overall accuracy remained high, the presence of poisoned samples showed that the model's decisions can be subtly changed, which is a serious problem for secure machine learning.

\end{document}
